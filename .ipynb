{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "459bda77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Train Data:\n",
      "                                                 title  \\\n",
      "0                                                        \n",
      "1                    muhammad life base earliest sourc   \n",
      "2                                                        \n",
      "3              destini disrupt histori world islam eye   \n",
      "4    reclaim heart person insight break free life s...   \n",
      "..                                                 ...   \n",
      "735              prophet pulpit commentari state islam   \n",
      "736                                                      \n",
      "737                                                      \n",
      "738  conflict fit islam america evolutionari psycholog   \n",
      "739                        scientif decept new atheist   \n",
      "\n",
      "                        author  \\\n",
      "0                       anonym   \n",
      "1                  martin ling   \n",
      "2    safiur rahman mubarakpuri   \n",
      "3                 tamim ansari   \n",
      "4                 yasmin mogah   \n",
      "..                         ...   \n",
      "735         khale abou el fadl   \n",
      "736                              \n",
      "737                              \n",
      "738                       amin   \n",
      "739                moham hijab   \n",
      "\n",
      "                                           description  \\\n",
      "0    quran english pronunci krn arab alqurn ipa qur...   \n",
      "1    martin ling biographi muhammad intern acclaim ...   \n",
      "2                                                        \n",
      "3    west share common narr world histori stori lar...   \n",
      "4    mani u live life entrap repeat pattern heartbr...   \n",
      "..                                                 ...   \n",
      "735  collect twentytwo islam sermon khale abou el f...   \n",
      "736                                                      \n",
      "737                                                      \n",
      "738  conflict fit islam america evolutionari psycho...   \n",
      "739  propon new atheist movement commonli use scien...   \n",
      "\n",
      "                                                genres  \\\n",
      "0    religion refer spiritu islam philosophi nonfic...   \n",
      "1    religion spiritu histori islam nonfict theolog...   \n",
      "2    religion spiritu histori unfinish islam nonfic...   \n",
      "3    religion audiobook world histori histori islam...   \n",
      "4    psycholog religion self help spiritu islam non...   \n",
      "..                                                 ...   \n",
      "735                                                set   \n",
      "736         audiobook nonfict islam self help religion   \n",
      "737                                     islam religion   \n",
      "738                   islam religion psycholog nonfict   \n",
      "739          nonfict islam atheism philosophi religion   \n",
      "\n",
      "                         pages  rating  \n",
      "0         604 pages, Hardcover    4.38  \n",
      "1         384 pages, Paperback    4.57  \n",
      "2         498 pages, Hardcover    4.60  \n",
      "3    416 pages, Kindle Edition    4.39  \n",
      "4         142 pages, Paperback    4.41  \n",
      "..                         ...     ...  \n",
      "735       265 pages, Paperback    4.65  \n",
      "736       216 pages, Paperback    4.40  \n",
      "737                  142 pages    4.50  \n",
      "738                      ebook    4.18  \n",
      "739        36 pages, Paperback    4.17  \n",
      "\n",
      "[740 rows x 6 columns]\n",
      "\n",
      "Preprocessed Test Data:\n",
      "                              title                  author  \\\n",
      "0                    mere christian                  c lewi   \n",
      "1                   screwtap letter                  c lewi   \n",
      "2                      great divorc                  c lewi   \n",
      "3                           confess          augustin hippo   \n",
      "4                      problem pain                  c lewi   \n",
      "..                              ...                     ...   \n",
      "787             black theolog liber             jame h cone   \n",
      "788                    uniti christ  saint cyril alexandria   \n",
      "789                red heroic rescu              ted dekker   \n",
      "790                      end affair            graham green   \n",
      "791  idol heart learn long god alon        elys fitzpatrick   \n",
      "\n",
      "                                           description  \\\n",
      "0    mere christian c lewi forc access doctrin chri...   \n",
      "1    screwtap letter c lewi classic masterpiec reli...   \n",
      "2    altern cover isbn c lewi great divorc classic ...   \n",
      "3    augustin confess one influenti innov work lati...   \n",
      "4    centuri peopl torment one question god good al...   \n",
      "..                                                 ...   \n",
      "787  messag relat liber poor societi christ messag ...   \n",
      "788  earli fifth centuri christian world rack one f...   \n",
      "789  step cliff fall madnessth mindbend pace black ...   \n",
      "790  stori begin end arbitrarili one choos moment e...   \n",
      "791  edit print updat revis edit releas isbn new ed...   \n",
      "\n",
      "                                                genres  \\\n",
      "0    theolog christian religion classic christian p...   \n",
      "1    theolog religion fantasi classic christian fic...   \n",
      "2    theolog religion fantasi classic fiction chris...   \n",
      "3    theolog christian religion classic philosophi ...   \n",
      "4    theolog christian religion christian philosoph...   \n",
      "..                                                 ...   \n",
      "787  spiritu religion theolog christian race social...   \n",
      "788  religion christian theolog christian classic n...   \n",
      "789  christian fiction scienc fiction thriller susp...   \n",
      "790  audiobook literatur romanc novel classic ficti...   \n",
      "791  christian christian live theolog counsel chris...   \n",
      "\n",
      "                         pages  rating  \n",
      "0     98 pages, Kindle Edition    4.35  \n",
      "1    222 pages, Kindle Edition    4.26  \n",
      "2         146 pages, Paperback    4.30  \n",
      "3         341 pages, Paperback    3.95  \n",
      "4         162 pages, Paperback    4.12  \n",
      "..                         ...     ...  \n",
      "787       214 pages, Paperback    4.21  \n",
      "788       151 pages, Paperback    4.32  \n",
      "789       381 pages, Paperback    4.28  \n",
      "790       160 pages, Paperback    3.90  \n",
      "791       239 pages, Paperback    4.34  \n",
      "\n",
      "[792 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#PREPROCESS STEP \n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# DATA \n",
    "train_data = pd.read_csv(\"C:\\\\Users\\\\nh013\\\\Desktop\\\\Religious Books (Islam and Christanity)\\\\books_Islam.csv\")\n",
    "test_data = pd.read_csv(\"C:\\\\Users\\\\nh013\\\\Desktop\\\\Religious Books (Islam and Christanity)\\\\books_Christanity.csv\")\n",
    "\n",
    "# FUNCTION TO PREPROCESS TEXT\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  # Check if the input is a string\n",
    "        #REMOVE URLS\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "        # REMOVE SPECIAL CHERECTER AND CONVERT TO LOWER CASE\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "\n",
    "        # TOKENIZE TEXT INTO WORDS\n",
    "        words = nltk.word_tokenize(text)\n",
    "\n",
    "        # REMOVE STOP WORDS\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "\n",
    "        # DEFINE STEMMER AND LEMMATIZER\n",
    "        stemmer = PorterStemmer()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        # APPLY STEMMER AND LEMMATIZER\n",
    "        words = [stemmer.stem(lemmatizer.lemmatize(word)) for word in words]\n",
    "\n",
    "        # JOIN THE WORDS BACKINTO A SINGLE STRING  \n",
    "        processed_text = ' '.join(words)\n",
    "\n",
    "        return processed_text\n",
    "    else:\n",
    "        return text  # RETURN THE INPUT UNCHANGED IF IT'S NOT A STRING  \n",
    "    \n",
    "# PERFORM PREPROCESSING TO TRAIN AND TEST DATAFRAME \n",
    "train_data['title'] = train_data['title'].apply(preprocess_text)\n",
    "train_data['author'] = train_data['author'].apply(preprocess_text)\n",
    "train_data['description'] = train_data['description'].apply(preprocess_text)\n",
    "train_data['genres'] = train_data['genres'].apply(preprocess_text)\n",
    "\n",
    "test_data['title'] = test_data['title'].apply(preprocess_text)\n",
    "test_data['author'] = test_data['author'].apply(preprocess_text)\n",
    "test_data['description'] = test_data['description'].apply(preprocess_text)\n",
    "test_data['genres'] = test_data['genres'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "print(\"Preprocessed Train Data:\")\n",
    "print(train_data)\n",
    "\n",
    "print(\"\\nPreprocessed Test Data:\")\n",
    "print(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f629a4c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6045e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba75654d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in train_data:\n",
      "title          0\n",
      "author         0\n",
      "description    0\n",
      "genres         0\n",
      "pages          2\n",
      "rating         0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in test_data:\n",
      "title          0\n",
      "author         0\n",
      "description    0\n",
      "genres         0\n",
      "pages          0\n",
      "rating         0\n",
      "dtype: int64\n",
      "TF-IDF Vectorized Features for Train Data (Title):\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "TF-IDF Vectorized Features for Train Data (Author):\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "TF-IDF Vectorized Features for Train Data (Description):\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.14126582 ... 0.         0.         0.        ]]\n",
      "\n",
      "TF-IDF Vectorized Features for Train Data (Genres):\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "TF-IDF Vectorized Features for Test Data (Title):\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "TF-IDF Vectorized Features for Test Data (Author):\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "TF-IDF Vectorized Features for Test Data (Description):\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.89013844 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.16143928 0.        ]]\n",
      "\n",
      "TF-IDF Vectorized Features for Test Data (Genres):\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# DATA \n",
    "train_data = pd.read_csv(\"C:\\\\Users\\\\nh013\\\\Desktop\\\\Religious Books (Islam and Christanity)\\\\books_Islam.csv\")\n",
    "test_data = pd.read_csv(\"C:\\\\Users\\\\nh013\\\\Desktop\\\\Religious Books (Islam and Christanity)\\\\books_Christanity.csv\")\n",
    "\n",
    "# FUNCTION TO PREPROCESS TEXT\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  # CHECK IF THE INPUT IS STRING \n",
    "        # REMOVE URLS\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "        # REMOVE SPECIAL CHARACTERS AND CONVERT TO LOWER CASE\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "\n",
    "        # TOKENIZE TEXT INTO WORDS\n",
    "        words = nltk.word_tokenize(text)\n",
    "\n",
    "        # REMOVE STOP WORDS\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "\n",
    "        # DEFINE STEMMER AND LEMMATIZER\n",
    "        stemmer = PorterStemmer()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        # APPLY STEMMER AND LEMMATIZER\n",
    "        words = [stemmer.stem(lemmatizer.lemmatize(word)) for word in words]\n",
    "\n",
    "        # JOIN THE WORDS BACK INTO A SINGLE STRING  \n",
    "        processed_text = ' '.join(words)\n",
    "\n",
    "        return processed_text\n",
    "    else:\n",
    "        return \"\"  \n",
    "\n",
    "# PERFORM PREPROCESSING TO TRAIN AND TEST DATAFRAME \n",
    "train_data['title'] = train_data['title'].apply(preprocess_text)\n",
    "train_data['author'] = train_data['author'].apply(preprocess_text)\n",
    "train_data['description'] = train_data['description'].apply(preprocess_text)\n",
    "train_data['genres'] = train_data['genres'].apply(preprocess_text)\n",
    "\n",
    "test_data['title'] = test_data['title'].apply(preprocess_text)\n",
    "test_data['author'] = test_data['author'].apply(preprocess_text)\n",
    "test_data['description'] = test_data['description'].apply(preprocess_text)\n",
    "test_data['genres'] = test_data['genres'].apply(preprocess_text)\n",
    "\n",
    "# TEXT VECTORIZATION USING TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, use_idf=True)\n",
    "tfidf_train_title = tfidf_vectorizer.fit_transform(train_data['title'])\n",
    "tfidf_train_author = tfidf_vectorizer.fit_transform(train_data['author'])\n",
    "tfidf_train_description = tfidf_vectorizer.fit_transform(train_data['description'])\n",
    "tfidf_train_genres = tfidf_vectorizer.fit_transform(train_data['genres'])\n",
    "\n",
    "tfidf_test_title = tfidf_vectorizer.transform(test_data['title'])\n",
    "tfidf_test_author = tfidf_vectorizer.transform(test_data['author'])\n",
    "tfidf_test_description = tfidf_vectorizer.transform(test_data['description'])\n",
    "tfidf_test_genres = tfidf_vectorizer.transform(test_data['genres'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Missing values in train_data:\")\n",
    "print(train_data.isnull().sum())\n",
    "\n",
    "\n",
    "print(\"\\nMissing values in test_data:\")\n",
    "print(test_data.isnull().sum())\n",
    "\n",
    "\n",
    "train_data.dropna(subset=['title', 'author', 'description','pages', 'genres'], inplace=True)\n",
    "\n",
    "test_data.dropna(subset=['title', 'author', 'description', 'genres'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"TF-IDF Vectorized Features for Train Data (Title):\")\n",
    "print(tfidf_train_title.toarray())\n",
    "\n",
    "print(\"\\nTF-IDF Vectorized Features for Train Data (Author):\")\n",
    "print(tfidf_train_author.toarray())\n",
    "\n",
    "print(\"\\nTF-IDF Vectorized Features for Train Data (Description):\")\n",
    "print(tfidf_train_description.toarray())\n",
    "\n",
    "print(\"\\nTF-IDF Vectorized Features for Train Data (Genres):\")\n",
    "print(tfidf_train_genres.toarray())\n",
    "\n",
    "print(\"\\nTF-IDF Vectorized Features for Test Data (Title):\")\n",
    "print(tfidf_test_title.toarray())\n",
    "\n",
    "print(\"\\nTF-IDF Vectorized Features for Test Data (Author):\")\n",
    "print(tfidf_test_author.toarray())\n",
    "\n",
    "print(\"\\nTF-IDF Vectorized Features for Test Data (Description):\")\n",
    "print(tfidf_test_description.toarray())\n",
    "\n",
    "print(\"\\nTF-IDF Vectorized Features for Test Data (Genres):\")\n",
    "print(tfidf_test_genres.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381e15c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6ef8ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d55097b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "24/24 [==============================] - 101s 4s/step - loss: 3.7892 - val_loss: 0.1124\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - 70s 3s/step - loss: 0.1109 - val_loss: 0.0464\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - 70s 3s/step - loss: 0.0961 - val_loss: 0.0410\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - 70s 3s/step - loss: 0.0890 - val_loss: 0.0410\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - 72s 3s/step - loss: 0.0906 - val_loss: 0.0450\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - 72s 3s/step - loss: 0.0908 - val_loss: 0.0417\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - 72s 3s/step - loss: 0.0886 - val_loss: 0.0478\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - 73s 3s/step - loss: 0.0865 - val_loss: 0.0450\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - 68s 3s/step - loss: 0.0848 - val_loss: 0.0525\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - 65s 3s/step - loss: 0.0808 - val_loss: 0.0549\n",
      "25/25 [==============================] - 16s 637ms/step - loss: 0.0549\n",
      "Test loss: 0.054866790771484375\n",
      "25/25 [==============================] - 18s 619ms/step\n",
      "                                                 title  predicted_rating\n",
      "197            book mormon anoth testament jesu christ          4.258873\n",
      "118                                    dark night soul          4.258873\n",
      "393                                     case real jesu          4.258873\n",
      "139                          dont enough faith atheist          4.258873\n",
      "760                                                             4.258873\n",
      "..                                                 ...               ...\n",
      "570     next evangel free church western cultur captiv          4.011279\n",
      "95             sever merci stori faith tragedi triumph          4.010810\n",
      "528  next right thing simpl soul practic make life ...          4.006959\n",
      "547                  abraham journey heart three faith          3.991318\n",
      "442  jesu realli love gay christian pilgrimag searc...          3.980659\n",
      "\n",
      "[792 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Build a Recurrent Neural Network (RNN) model to predict book ratings and suggest books based on their ratings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# DATA\n",
    "train_data = pd.read_csv(\"C:\\\\Users\\\\nh013\\\\Desktop\\\\Religious Books (Islam and Christanity)\\\\books_Islam.csv\")\n",
    "test_data = pd.read_csv(\"C:\\\\Users\\\\nh013\\\\Desktop\\\\Religious Books (Islam and Christanity)\\\\books_Christanity.csv\")\n",
    "\n",
    "# FUNCTION TO PREPROCESS TEXT\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "        words = nltk.word_tokenize(text)\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "        stemmer = PorterStemmer()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = [stemmer.stem(lemmatizer.lemmatize(word)) for word in words]\n",
    "        processed_text = ' '.join(words)\n",
    "        return processed_text\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# PERFORM PREPROCESS TO TRAIN AND TEST SET\n",
    "train_data['title'] = train_data['title'].apply(preprocess_text)\n",
    "train_data['author'] = train_data['author'].apply(preprocess_text)\n",
    "train_data['description'] = train_data['description'].apply(preprocess_text)\n",
    "train_data['genres'] = train_data['genres'].apply(preprocess_text)\n",
    "\n",
    "test_data['title'] = test_data['title'].apply(preprocess_text)\n",
    "test_data['author'] = test_data['author'].apply(preprocess_text)\n",
    "test_data['description'] = test_data['description'].apply(preprocess_text)\n",
    "test_data['genres'] = test_data['genres'].apply(preprocess_text)\n",
    "\n",
    "# TOKENIZATION\n",
    "max_words = 1000  \n",
    "tokenizer = Tokenizer(num_words=max_words, split=' ')\n",
    "tokenizer.fit_on_texts(train_data['description'])\n",
    "X_train = tokenizer.texts_to_sequences(train_data['description'])\n",
    "X_test = tokenizer.texts_to_sequences(test_data['description'])\n",
    "\n",
    "#PADDING SEQUENCE\n",
    "max_sequence_length = 500  # Define the maximum sequence length\n",
    "X_train = pad_sequences(X_train, maxlen=max_sequence_length)\n",
    "X_test = pad_sequences(X_test, maxlen=max_sequence_length)\n",
    "\n",
    "#BUILD RNN MODEL\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_sequence_length))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='linear')) \n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# TRAIN MODEL TO PREDICT BOOK RATING \n",
    "y_train = train_data['rating']\n",
    "y_test = test_data['rating']\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n",
    "\n",
    "# EVALUATE MODEL ON THE TEST DATA\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {test_loss}')\n",
    "\n",
    "# MAKE PREDICTION ON TEST DATA\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# SUGGEST BOOKS BASED ON THEIR PREDICTION RATING \n",
    "suggested_books = test_data.copy()\n",
    "suggested_books['predicted_rating'] = predictions\n",
    "\n",
    "# SUGGESTES BOOKBY PREDICTES_RATING TO SUGGEST HIGHEST_RATED BOOKS \n",
    "suggested_books = suggested_books.sort_values(by='predicted_rating', ascending=False)\n",
    "\n",
    "\n",
    "print(suggested_books[['title', 'predicted_rating']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86f4483",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
